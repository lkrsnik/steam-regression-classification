{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bb44827-a885-4e93-8517-94e4244f6d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
    "import os\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "\n",
    "manual_seed = 23\n",
    "\n",
    "np.random.seed(manual_seed)\n",
    "pl.set_random_seed(manual_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77631e4-08f3-41df-914f-73dcdfb0bb1a",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79252084-e705-4fe0-ab67-cf8213cccc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/500k_50k'\n",
    "\n",
    "df_train = pl.read_parquet(filename + '_train.parquet')\n",
    "df_dev = pl.read_parquet(filename + '_dev.parquet')\n",
    "df_test = pl.read_parquet(filename + '_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d875723c-c08a-432a-9c76-1e80756527c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 6)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>product_id</th><th>review_text</th><th>recommended</th><th>found_awarding</th><th>found_helpful</th><th>found_funny</th></tr><tr><td>i64</td><td>str</td><td>i8</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>1057090</td><td>&quot;ya i&#x27;m gay&quot;</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>40340</td><td>&quot;Ran across a bug in the game w…</td><td>0</td><td>0.0</td><td>0.051619</td><td>0.0</td></tr><tr><td>244210</td><td>&quot;Best drifting game!&quot;</td><td>1</td><td>0.0</td><td>0.008</td><td>0.0</td></tr><tr><td>105450</td><td>&quot;Its da biz&quot;</td><td>1</td><td>0.0</td><td>0.008</td><td>0.0</td></tr><tr><td>200210</td><td>&quot;The reason i do not recomend t…</td><td>0</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>738520</td><td>&quot;If you like Portal and you lik…</td><td>1</td><td>0.130435</td><td>0.064</td><td>0.023529</td></tr><tr><td>326460</td><td>&quot;Good!&quot;</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>2080690</td><td>&quot;amazing game&quot;</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td></tr><tr><td>602960</td><td>&quot;Pretty good game. The Crewplay…</td><td>1</td><td>0.0</td><td>0.004</td><td>0.0</td></tr><tr><td>1091500</td><td>&quot;good game go pew pew&quot;</td><td>1</td><td>0.0</td><td>0.0</td><td>0.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 6)\n",
       "┌────────────┬────────────────────────┬─────────────┬────────────────┬───────────────┬─────────────┐\n",
       "│ product_id ┆ review_text            ┆ recommended ┆ found_awarding ┆ found_helpful ┆ found_funny │\n",
       "│ ---        ┆ ---                    ┆ ---         ┆ ---            ┆ ---           ┆ ---         │\n",
       "│ i64        ┆ str                    ┆ i8          ┆ f64            ┆ f64           ┆ f64         │\n",
       "╞════════════╪════════════════════════╪═════════════╪════════════════╪═══════════════╪═════════════╡\n",
       "│ 1057090    ┆ ya i'm gay             ┆ 1           ┆ 0.0            ┆ 0.0           ┆ 0.0         │\n",
       "│ 40340      ┆ Ran across a bug in    ┆ 0           ┆ 0.0            ┆ 0.051619      ┆ 0.0         │\n",
       "│            ┆ the game w…            ┆             ┆                ┆               ┆             │\n",
       "│ 244210     ┆ Best drifting game!    ┆ 1           ┆ 0.0            ┆ 0.008         ┆ 0.0         │\n",
       "│ 105450     ┆ Its da biz             ┆ 1           ┆ 0.0            ┆ 0.008         ┆ 0.0         │\n",
       "│ 200210     ┆ The reason i do not    ┆ 0           ┆ 0.0            ┆ 0.0           ┆ 0.0         │\n",
       "│            ┆ recomend t…            ┆             ┆                ┆               ┆             │\n",
       "│ 738520     ┆ If you like Portal and ┆ 1           ┆ 0.130435       ┆ 0.064         ┆ 0.023529    │\n",
       "│            ┆ you lik…               ┆             ┆                ┆               ┆             │\n",
       "│ 326460     ┆ Good!                  ┆ 1           ┆ 0.0            ┆ 0.0           ┆ 0.0         │\n",
       "│ 2080690    ┆ amazing game           ┆ 1           ┆ 0.0            ┆ 0.0           ┆ 0.0         │\n",
       "│ 602960     ┆ Pretty good game. The  ┆ 1           ┆ 0.0            ┆ 0.004         ┆ 0.0         │\n",
       "│            ┆ Crewplay…              ┆             ┆                ┆               ┆             │\n",
       "│ 1091500    ┆ good game go pew pew   ┆ 1           ┆ 0.0            ┆ 0.0           ┆ 0.0         │\n",
       "└────────────┴────────────────────────┴─────────────┴────────────────┴───────────────┴─────────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2cfb3d7-b814-45ff-ab79-d16130612488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upscale importance of 1+ funny\n",
    "df_train = df_train.with_columns((pl.col(\"found_funny\") + pl.col(\"found_funny\").ceil()) / 2)\n",
    "df_dev = df_dev.with_columns((pl.col(\"found_funny\") + pl.col(\"found_funny\").ceil()) / 2)\n",
    "df_test = df_test.with_columns((pl.col(\"found_funny\") + pl.col(\"found_funny\").ceil()) / 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b46c575-30ac-487d-89c8-3e4647bb508b",
   "metadata": {},
   "source": [
    "## Downsize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624d03d4-8460-4af4-b17c-7ac89ed36b34",
   "metadata": {},
   "source": [
    "The obtained dataset contains ~45M reviews. Training on this amount would take too long, so I decided to train models on smaller chunks of data. I aimed at something that wouldn't take more than 6 hours of training. For Roberta, this meant training on 500k randomly selected reviews. I decided to evaluate data on 50k reviews, which means 10% of the size of the training data. While the amount of training data might change, this evaluation set will be used for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d69c034-40d8-42dc-aab9-fddb62650a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roberta\n",
    "df_train = df_train.sample(5000, seed=manual_seed, shuffle=True)\n",
    "df_dev = df_dev.sample(500, seed=manual_seed, shuffle=True)\n",
    "df_test = df_test.sample(500, seed=manual_seed, shuffle=True)\n",
    "\n",
    "# TODO DELETE THIS BECAUSE PREPROCESSING!\n",
    "#df_train = df_train.cast({'recommended': pl.Int8})\n",
    "#df_dev = df_dev.cast({'recommended': pl.Int8})\n",
    "#df_test = df_test.cast({'recommended': pl.Int8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a461911-0398-4f36-9e44-b5b67639c524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (500, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>text</th><th>label</th></tr><tr><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;Good story but a little short.…</td><td>0.052541</td></tr><tr><td>&quot;- - - - implants are too expen…</td><td>0.004</td></tr><tr><td>&quot;eh idk i just like it&quot;</td><td>0.0</td></tr><tr><td>&quot;Smooth movement in game, graph…</td><td>0.0</td></tr><tr><td>&quot;What you see in the screenshot…</td><td>0.113143</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;Its a great experience&quot;</td><td>0.0</td></tr><tr><td>&quot;if you believe in sphere earth…</td><td>0.0</td></tr><tr><td>&quot;Played it for about 11 hours. …</td><td>0.012</td></tr><tr><td>&quot;Pay to win game. as it&#x27;s mostl…</td><td>0.013732</td></tr><tr><td>&quot;connection with server was los…</td><td>0.004</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (500, 2)\n",
       "┌─────────────────────────────────┬──────────┐\n",
       "│ text                            ┆ label    │\n",
       "│ ---                             ┆ ---      │\n",
       "│ str                             ┆ f64      │\n",
       "╞═════════════════════════════════╪══════════╡\n",
       "│ Good story but a little short.… ┆ 0.052541 │\n",
       "│ - - - - implants are too expen… ┆ 0.004    │\n",
       "│ eh idk i just like it           ┆ 0.0      │\n",
       "│ Smooth movement in game, graph… ┆ 0.0      │\n",
       "│ What you see in the screenshot… ┆ 0.113143 │\n",
       "│ …                               ┆ …        │\n",
       "│ Its a great experience          ┆ 0.0      │\n",
       "│ if you believe in sphere earth… ┆ 0.0      │\n",
       "│ Played it for about 11 hours. … ┆ 0.012    │\n",
       "│ Pay to win game. as it's mostl… ┆ 0.013732 │\n",
       "│ connection with server was los… ┆ 0.004    │\n",
       "└─────────────────────────────────┴──────────┘"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f088cead-20f2-4e23-974c-4408c3491c14",
   "metadata": {},
   "source": [
    "## Selecting relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32a2a13b-746c-42dd-9c29-2794c4a55001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommended\n",
    "# df_train = df_train.select(['review_text', 'recommended']).rename({'review_text': 'text', 'recommended': 'label'})\n",
    "# df_dev = df_dev.select(['review_text', 'recommended']).rename({'review_text': 'text', 'recommended': 'label'})\n",
    "# df_test = df_test.select(['review_text', 'recommended']).rename({'review_text': 'text', 'recommended': 'label'})\n",
    "\n",
    "# found_helpful\n",
    "df_train = df_train.select(['review_text', 'found_helpful']).rename({'review_text': 'text', 'found_helpful': 'label'})\n",
    "df_dev = df_dev.select(['review_text', 'found_helpful']).rename({'review_text': 'text', 'found_helpful': 'label'})\n",
    "df_test = df_test.select(['review_text', 'found_helpful']).rename({'review_text': 'text', 'found_helpful': 'label'})\n",
    "\n",
    "# found funny\n",
    "# df_train = df_train.select(['review_text', 'found_funny']).rename({'review_text': 'text', 'found_funny': 'label'})\n",
    "# df_dev = df_dev.select(['review_text', 'found_funny']).rename({'review_text': 'text', 'found_funny': 'label'})\n",
    "# df_test = df_test.select(['review_text', 'found_funny']).rename({'review_text': 'text', 'found_funny': 'label'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9706b26-1105-469e-8311-1b92f9d47913",
   "metadata": {},
   "source": [
    "## Create dataset for transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12f13b30-249f-4421-9ea9-4dfc51f7f9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\n",
    "    'train': Dataset(df_train.to_arrow()),\n",
    "    'dev': Dataset(df_dev.to_arrow()),\n",
    "    'test': Dataset(df_test.to_arrow())\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f46c88e6-59a7-4393-bef9-f36b3cd3b06c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 500000\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c124a47-6e20-4be7-86ff-34538876eab1",
   "metadata": {},
   "source": [
    "# Training -  - simpletransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d6aeb4-40c3-47f0-b892-d08e85130da7",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b98e9123-921a-4aaa-a2e7-04c99dc0683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup classification arguments\n",
    "classification_args = {\n",
    "    'num_train_epochs': 1,\n",
    "    'manual_seed': manual_seed,\n",
    "    'save_steps': -1,\n",
    "    'train_batch_size': 32\n",
    "}\n",
    "\n",
    "model_args = ClassificationArgs(**classification_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455a5f7b-7cdc-4c36-9b74-f4449abd962d",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71cfa8cd-9424-415d-8ab6-0973aa06763b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/luka/Development/personal/steam-experiments/venv2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# setup model\n",
    "# model_args = {\n",
    "#     'model_type': 'roberta',\n",
    "#     'model_name': 'models/roberta500k/model/checkpoint-15625-epoch-1',\n",
    "#     'num_labels': 2,\n",
    "#     'args': model_args\n",
    "# }\n",
    "model_args = {\n",
    "    'model_type': 'distilbert',\n",
    "    'model_name': 'distilbert/distilbert-base-uncased',\n",
    "    'num_labels': 2,\n",
    "    'args': model_args\n",
    "}\n",
    "model = ClassificationModel(**model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d989111-450f-410f-addc-d57167267af8",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6db373a-a136-4437-8bbb-c9478e643a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luka/Development/personal/steam-experiments/venv2/lib/python3.10/site-packages/simpletransformers/classification/classification_model.py:610: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bcb9315d21c4bdabfd91c0d58b5f690",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8354f2db68d46cfaf62dcbb250a4ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luka/Development/personal/steam-experiments/venv2/lib/python3.10/site-packages/simpletransformers/classification/classification_model.py:882: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = amp.GradScaler()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "912bcea665ba49028e1f8136ae489cd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 1 of 1:   0%|          | 0/15625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luka/Development/personal/steam-experiments/venv2/lib/python3.10/site-packages/simpletransformers/classification/classification_model.py:905: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15625, 0.1697815614566803)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.train_model(df_train.to_pandas(), output_dir='models/roberta500k')\n",
    "model.train_model(df_train.to_pandas(), output_dir='models/distilbert500k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f850681a-2cce-495d-9d72-261dd7609ec6",
   "metadata": {},
   "source": [
    "# Training - transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2b5011b-e1f0-41ea-82fb-4e9a24396a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e62fec6b-b1dc-46a2-bb7b-8f89bb6d914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations distilbert - recommended\n",
    "model_type = 'distilbert'\n",
    "model_name = 'distilbert/distilbert-base-uncased'\n",
    "output_dir='models/steam-classification-distilbert500k'\n",
    "batch_size = 32\n",
    "num_epochs = 1\n",
    "lr = 5e-5 # default\n",
    "weight_decay = 0\n",
    "eval_steps=0.1 # eval after 10% is done\n",
    "save_steps=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12fc336d-fc49-40f3-8108-b151b6d3b828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations distilbert - helpful\n",
    "model_type = 'distilbert'\n",
    "model_name = 'distilbert/distilbert-base-uncased'\n",
    "output_dir='models/steam-classification-distilbert500k-helpful'\n",
    "batch_size = 32\n",
    "num_epochs = 1\n",
    "lr = 5e-5 # default\n",
    "weight_decay = 0\n",
    "eval_steps=0.1 # eval after 10% is done\n",
    "save_steps=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d35da449-2ea5-4aa6-b372-021ed622e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations distilbert - funny\n",
    "model_type = 'distilbert'\n",
    "model_name = 'distilbert/distilbert-base-uncased'\n",
    "output_dir='models/steam-classification-distilbert500k-funny2'\n",
    "batch_size = 32\n",
    "num_epochs = 1\n",
    "lr = 5e-5 # default\n",
    "weight_decay = 0\n",
    "eval_steps=0.1 # eval after 10% is done\n",
    "save_steps=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa88ebe4-75a8-45dd-bc7f-97db6f9e9c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations distilbert - funny\n",
    "model_type = 'distilbert'\n",
    "model_name = 'distilbert/distilbert-base-uncased'\n",
    "output_dir='models/steam-classification-distilbert500k-funny3'\n",
    "batch_size = 32\n",
    "num_epochs = 1\n",
    "lr = 5e-6 # default\n",
    "weight_decay = 0\n",
    "eval_steps=0.1 # eval after 10% is done\n",
    "save_steps=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ff78771-2df7-4680-bc62-73e8c2d32a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations roberta - recommended\n",
    "model_type = 'roberta-large'\n",
    "model_name = 'FacebookAI/roberta-large'\n",
    "output_dir='models/steam-classification-roberta500k'\n",
    "batch_size = 16\n",
    "num_epochs = 1\n",
    "lr = 5e-6 # lower than default\n",
    "weight_decay = 0\n",
    "eval_steps=0.1 # eval after 10% is done\n",
    "save_steps=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6194175-5e96-4bc6-a0a9-9d61479b6ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations roberta - helpful\n",
    "model_type = 'roberta-large'\n",
    "model_name = 'FacebookAI/roberta-large'\n",
    "output_dir='models/steam-classification-roberta500k-helpful'\n",
    "batch_size = 16\n",
    "num_epochs = 1\n",
    "lr = 5e-6 # lower than default\n",
    "weight_decay = 0\n",
    "eval_steps=0.1 # eval after 10% is done\n",
    "save_steps=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca15bd70-b0a0-4cef-9406-7d37c6b6154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations roberta - funny\n",
    "model_type = 'roberta-large'\n",
    "model_name = 'FacebookAI/roberta-large'\n",
    "output_dir='models/steam-classification-roberta500k-funny'\n",
    "batch_size = 16\n",
    "num_epochs = 1\n",
    "lr = 5e-6 # lower than default\n",
    "weight_decay = 0\n",
    "eval_steps=0.1 # eval after 10% is done\n",
    "save_steps=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a244143-77d9-4ba1-98f5-eeacc80a8a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luka/Development/personal/steam-experiments/venv2/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# classification\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# regression\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    text = examples[\"text\"]\n",
    "    # it is possible to return tensors in pytorch, but then you need to pad everything which is inconvenient because it is better to do in collator\n",
    "    return tokenizer(text, truncation=True, return_tensors=\"np\", max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5d98950-cf5b-4736-9f47-89f167f784ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edfeb0b9e18f4cbb9025cb04cce2d631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9974a7df79b74386a3d3b0233b34f312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec503b67fca141ddb1a263eb1406214e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "814b3f41-dd97-4df2-ae7b-9d28002c83fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b66cdf03-be6c-40e4-8e4d-3b1e77e77a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels)['accuracy']}\n",
    "    # return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8d28312-d1fc-434f-b361-9152448c8fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy import stats\n",
    "\n",
    "spearmanr_func = lambda x, y: stats.spearmanr(x, y)[0]\n",
    "pearsonr_func = lambda x, y: stats.pearsonr(x, y)[0]\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    #spearmanr = spearmanr_func(predictions, labels)\n",
    "    #pearsonr = pearsonr_func(predictions, labels)\n",
    "    labels = labels.reshape(-1, 1)\n",
    "    mse = mean_squared_error(labels, predictions)\n",
    "    mae = mean_absolute_error(labels, predictions)\n",
    "    r2 = r2_score(labels, predictions)\n",
    "    \n",
    "    return {\"mse\": mse, \"mae\": mae, \"r2\": r2} #, \"spearmanr\": spearmanr, \"pearsonr\": pearsonr}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ea9ddb-b9c4-41b0-bb2e-900f9767c029",
   "metadata": {},
   "source": [
    "## Test untrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4e17f49-bfc5-43c1-b1cb-59f5bde7f70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 = I totally agree the best bit of the sims making dream homes, but really needs the ability to sell and make a profit so you can buy more land and build your housing empire. Love the designs and the possibilities for the game\n",
      "0 = Banger game cheap when on sale comes with a lot of dlc! Start the exe from the main folder and it won't crash because \"ran out of memory\" as much and enjoy pasting all those codes for the keys. I think 7 hours out of my 12 is just pasting codes.\n",
      "0 = Too much random ♥♥♥♥♥♥♥♥.\n",
      "0 = Story - 8\n",
      "Visuals - 9\n",
      "Audio - 9\n",
      "Gameplay - 8\n",
      "Length - 6\n",
      "Replayability - 7\n",
      "Value base $ - 6\n",
      "Value sale ($3.74) - 8\n",
      "Overall - 8\n",
      "0 = This game is awesome. Play it.\n",
      "0 = Great story, a visual novel walking simulator which is fueled by the modern non-organic extraterrestrial travelling theories. Better than most movies i guess..\n",
      "Oh! It ran well, with full details with my 7 years old 1070GTX card, exhibiting me a visually stunning red planet\n",
      "0 = amazing game the artstyle is creepy the controls are awesome and basically everything is cool\n",
      "0 = Some really tedious game mechanics (even by RGG standards - e.g. endless tailing missions, fiddling with keys, etc.) with mostly below average side quests. However, the main story and the accompanying soundtrack is excellent and more than makes up for these flaws. This would probably be an even better game overall if it wasn't chained back by the usual RGG elements which did little to improve the quality of the game anyway.\n",
      "0 = I really enjoy this game. If you want a FPS that takes elements of CoD and CSGO, and puts them into a F2P game, this is worth checking out. Might not be perfect, but I mean c'mon it's still early access. Play it, and take it for what it is. It is still a developing game. It has great potential. It runs well on my old toaster laptop, has fun game modes, and balanced guns, cool maps, so I vote yes for Black Sqaud, despite its minor flaws.\n",
      "0 = > Got the game out of pure curiosity\n",
      "> Hmm, the artwork and soundtrack are pretty nice\n",
      "> Get outside for the first time\n",
      "> HOLY ♥♥♥♥♥ IT'S SO MESMERIZING AND SORROWFUL AT THE SAME TIME, WTH\n",
      "> Yeah, Mili did the soundtrack, so that's why\n",
      "> Tbh, the combat and exploration is not that varied, huh\n",
      "> Regret saying so cuz later, the map is so large and overwhelming that it made me feel helpless at times\n",
      "> The combat got so much better after each boss fight. Srsly, progressing in this game feels so satisfying.\n",
      "> The story and the lore... man... it adds so much to the game. I just read the some left behind notes and im sad af rn.\n",
      "> At some point, reach Verboten Domain\n",
      "> Feel the dread and fear constantly\n",
      "> Reach the Abyss\n",
      "> SH** MY PANTS WHILE SUFFOCATING IN THE GAS CHAMBER\n",
      "> Reach final boss\n",
      "> Cry\n",
      "10/10 would recommend playing it all again\n"
     ]
    }
   ],
   "source": [
    "def example_predictions(dataset, model):\n",
    "    for text in dataset:\n",
    "        inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "        logits = model(inputs).logits\n",
    "        predictions = torch.argmax(logits)\n",
    "    \n",
    "        print(f'{predictions.tolist()} = {text}')\n",
    "example_predictions(tokenized_dataset['dev']['text'][:10], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "765ff4ee-f1c5-4a67-bff2-07ff850700e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LORA CONFIG - IGNORE FOR NOW!\n",
    "# peft_config = LoraConfig(task_type=\"SEQ_CLS\", # sequence classification\n",
    "#                          r=4, # intrinsic rank of trainable weight matrix\n",
    "#                          lora_alpha=32, # like a learning rate\n",
    "#                          lora_dropout=0.01, # dropout probability\n",
    "#                          target_modules = ['q_lin']) # apply lora to query layer\n",
    "\n",
    "# model = get_peft_model(model, peft_config)\n",
    "# model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb13d852-1194-4168-a146-11d4539c2c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=lr,\n",
    "    weight_decay=weight_decay,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=eval_steps, # eval after 10% is done\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=save_steps, # save after 10% of processing is done\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "258d4aa2-f933-4858-8c0a-80128c6361b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['dev'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26d2fcc2-ec56-4a70-9dbf-6a20f033963b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'args': TrainingArguments(\n",
       " _n_gpu=1,\n",
       " accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
       " adafactor=False,\n",
       " adam_beta1=0.9,\n",
       " adam_beta2=0.999,\n",
       " adam_epsilon=1e-08,\n",
       " auto_find_batch_size=False,\n",
       " batch_eval_metrics=False,\n",
       " bf16=False,\n",
       " bf16_full_eval=False,\n",
       " data_seed=None,\n",
       " dataloader_drop_last=False,\n",
       " dataloader_num_workers=0,\n",
       " dataloader_persistent_workers=False,\n",
       " dataloader_pin_memory=True,\n",
       " dataloader_prefetch_factor=None,\n",
       " ddp_backend=None,\n",
       " ddp_broadcast_buffers=None,\n",
       " ddp_bucket_cap_mb=None,\n",
       " ddp_find_unused_parameters=None,\n",
       " ddp_timeout=1800,\n",
       " debug=[],\n",
       " deepspeed=None,\n",
       " disable_tqdm=False,\n",
       " dispatch_batches=None,\n",
       " do_eval=True,\n",
       " do_predict=False,\n",
       " do_train=False,\n",
       " eval_accumulation_steps=None,\n",
       " eval_delay=0,\n",
       " eval_do_concat_batches=True,\n",
       " eval_on_start=False,\n",
       " eval_steps=0.1,\n",
       " eval_strategy=steps,\n",
       " eval_use_gather_object=False,\n",
       " evaluation_strategy=None,\n",
       " fp16=False,\n",
       " fp16_backend=auto,\n",
       " fp16_full_eval=False,\n",
       " fp16_opt_level=O1,\n",
       " fsdp=[],\n",
       " fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
       " fsdp_min_num_params=0,\n",
       " fsdp_transformer_layer_cls_to_wrap=None,\n",
       " full_determinism=False,\n",
       " gradient_accumulation_steps=1,\n",
       " gradient_checkpointing=False,\n",
       " gradient_checkpointing_kwargs=None,\n",
       " greater_is_better=False,\n",
       " group_by_length=False,\n",
       " half_precision_backend=auto,\n",
       " hub_always_push=False,\n",
       " hub_model_id=None,\n",
       " hub_private_repo=False,\n",
       " hub_strategy=every_save,\n",
       " hub_token=<HUB_TOKEN>,\n",
       " ignore_data_skip=False,\n",
       " include_inputs_for_metrics=False,\n",
       " include_num_input_tokens_seen=False,\n",
       " include_tokens_per_second=False,\n",
       " jit_mode_eval=False,\n",
       " label_names=None,\n",
       " label_smoothing_factor=0.0,\n",
       " learning_rate=4e-05,\n",
       " length_column_name=length,\n",
       " load_best_model_at_end=True,\n",
       " local_rank=0,\n",
       " log_level=passive,\n",
       " log_level_replica=warning,\n",
       " log_on_each_node=True,\n",
       " logging_dir=models/steam-classification-roberta500k/runs/Sep14_10-13-07_luka-PC,\n",
       " logging_first_step=False,\n",
       " logging_nan_inf_filter=True,\n",
       " logging_steps=500,\n",
       " logging_strategy=steps,\n",
       " lr_scheduler_kwargs={},\n",
       " lr_scheduler_type=linear,\n",
       " max_grad_norm=1.0,\n",
       " max_steps=-1,\n",
       " metric_for_best_model=loss,\n",
       " mp_parameters=,\n",
       " neftune_noise_alpha=None,\n",
       " no_cuda=False,\n",
       " num_train_epochs=1,\n",
       " optim=adamw_torch,\n",
       " optim_args=None,\n",
       " optim_target_modules=None,\n",
       " output_dir=models/steam-classification-roberta500k,\n",
       " overwrite_output_dir=False,\n",
       " past_index=-1,\n",
       " per_device_eval_batch_size=32,\n",
       " per_device_train_batch_size=32,\n",
       " prediction_loss_only=False,\n",
       " push_to_hub=False,\n",
       " push_to_hub_model_id=None,\n",
       " push_to_hub_organization=None,\n",
       " push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
       " ray_scope=last,\n",
       " remove_unused_columns=True,\n",
       " report_to=['tensorboard', 'wandb'],\n",
       " restore_callback_states_from_checkpoint=False,\n",
       " resume_from_checkpoint=None,\n",
       " run_name=models/steam-classification-roberta500k,\n",
       " save_on_each_node=False,\n",
       " save_only_model=False,\n",
       " save_safetensors=True,\n",
       " save_steps=0.1,\n",
       " save_strategy=steps,\n",
       " save_total_limit=None,\n",
       " seed=42,\n",
       " skip_memory_metrics=True,\n",
       " split_batches=None,\n",
       " tf32=None,\n",
       " torch_compile=False,\n",
       " torch_compile_backend=None,\n",
       " torch_compile_mode=None,\n",
       " torch_empty_cache_steps=None,\n",
       " torchdynamo=None,\n",
       " tpu_metrics_debug=False,\n",
       " tpu_num_cores=None,\n",
       " use_cpu=False,\n",
       " use_ipex=False,\n",
       " use_legacy_prediction_loop=False,\n",
       " use_mps_device=False,\n",
       " warmup_ratio=0.0,\n",
       " warmup_steps=0,\n",
       " weight_decay=0,\n",
       " ),\n",
       " 'hp_name': None,\n",
       " 'deepspeed': None,\n",
       " 'is_in_train': False,\n",
       " 'accelerator': <accelerate.accelerator.Accelerator at 0x74674a4afa90>,\n",
       " 'gather_function': functools.partial(<bound method Accelerator.gather_for_metrics of <accelerate.accelerator.Accelerator object at 0x74674a4afa90>>, use_gather_object=False),\n",
       " 'is_deepspeed_enabled': False,\n",
       " 'is_fsdp_enabled': False,\n",
       " '_memory_tracker': <transformers.trainer_utils.TrainerMemoryTracker at 0x74674a36a050>,\n",
       " 'model_init': None,\n",
       " 'is_model_parallel': False,\n",
       " 'is_fsdp_xla_enabled': False,\n",
       " 'place_model_on_device': True,\n",
       " 'data_collator': DataCollatorWithPadding(tokenizer=RobertaTokenizerFast(name_or_path='FacebookAI/roberta-large', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       " \t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       " }, padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt'),\n",
       " 'train_dataset': Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 500000\n",
       " }),\n",
       " 'eval_dataset': Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 50000\n",
       " }),\n",
       " 'tokenizer': RobertaTokenizerFast(name_or_path='FacebookAI/roberta-large', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       " \t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       " \t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       " },\n",
       " 'model_wrapped': RobertaForSequenceClassification(\n",
       "   (roberta): RobertaModel(\n",
       "     (embeddings): RobertaEmbeddings(\n",
       "       (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "       (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "       (token_type_embeddings): Embedding(1, 1024)\n",
       "       (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (encoder): RobertaEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0-23): 24 x RobertaLayer(\n",
       "           (attention): RobertaAttention(\n",
       "             (self): RobertaSelfAttention(\n",
       "               (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "               (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "               (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): RobertaSelfOutput(\n",
       "               (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "               (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): RobertaIntermediate(\n",
       "             (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "             (intermediate_act_fn): GELUActivation()\n",
       "           )\n",
       "           (output): RobertaOutput(\n",
       "             (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "             (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (classifier): RobertaClassificationHead(\n",
       "     (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
       "   )\n",
       " ),\n",
       " 'model': RobertaForSequenceClassification(\n",
       "   (roberta): RobertaModel(\n",
       "     (embeddings): RobertaEmbeddings(\n",
       "       (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "       (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "       (token_type_embeddings): Embedding(1, 1024)\n",
       "       (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (encoder): RobertaEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0-23): 24 x RobertaLayer(\n",
       "           (attention): RobertaAttention(\n",
       "             (self): RobertaSelfAttention(\n",
       "               (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "               (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "               (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): RobertaSelfOutput(\n",
       "               (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "               (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): RobertaIntermediate(\n",
       "             (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "             (intermediate_act_fn): GELUActivation()\n",
       "           )\n",
       "           (output): RobertaOutput(\n",
       "             (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "             (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (classifier): RobertaClassificationHead(\n",
       "     (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "     (dropout): Dropout(p=0.1, inplace=False)\n",
       "     (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
       "   )\n",
       " ),\n",
       " 'neftune_noise_alpha': None,\n",
       " 'compute_metrics': <function __main__.compute_metrics(eval_pred)>,\n",
       " 'preprocess_logits_for_metrics': None,\n",
       " 'optimizer': None,\n",
       " 'lr_scheduler': None,\n",
       " 'callback_handler': <transformers.trainer_callback.CallbackHandler at 0x74674a3be2c0>,\n",
       " '_loggers_initialized': False,\n",
       " 'hub_model_id': None,\n",
       " '_signature_columns': None,\n",
       " 'use_apex': False,\n",
       " 'use_cpu_amp': False,\n",
       " 'label_smoother': None,\n",
       " 'control': TrainerControl(should_training_stop=False, should_epoch_stop=False, should_save=False, should_evaluate=False, should_log=False),\n",
       " 'state': TrainerState(epoch=None, global_step=0, max_steps=0, logging_steps=500, eval_steps=500, save_steps=500, train_batch_size=None, num_train_epochs=0, num_input_tokens_seen=0, total_flos=0, log_history=[], best_metric=None, best_model_checkpoint=None, is_local_process_zero=True, is_world_process_zero=True, is_hyper_param_search=False, trial_name=None, trial_params=None, stateful_callbacks={'TrainerControl': {'args': {'should_training_stop': False, 'should_epoch_stop': False, 'should_save': False, 'should_evaluate': False, 'should_log': False}, 'attributes': {}}}),\n",
       " 'current_flos': 0,\n",
       " 'hp_search_backend': None,\n",
       " 'label_names': ['labels'],\n",
       " 'can_return_loss': False,\n",
       " '_train_batch_size': 32,\n",
       " '_created_lr_scheduler': False,\n",
       " 'is_fsdp_xla_v2_enabled': False}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd9cfd88-c79a-4258-9717-840b333c57c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luka/Development/personal/steam-experiments/venv2/lib/python3.10/site-packages/transformers/trainer.py:3098: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(os.path.join(checkpoint, OPTIMIZER_NAME), map_location=map_location)\n",
      "Warning: The following arguments do not match the ones in the `trainer_state.json` within the checkpoint directory: \n",
      "\teval_steps: 0.1 (from args) != 3125 (from trainer_state.json)\n",
      "\tsave_steps: 0.1 (from args) != 3125 (from trainer_state.json)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mluka-krsnik\u001b[0m (\u001b[33mluka-krsnik-outsmartify\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/luka/Development/personal/steam-experiments/steam-regression-classification/wandb/run-20240924_084635-o0no16wd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/luka-krsnik-outsmartify/huggingface/runs/o0no16wd' target=\"_blank\">models/steam-classification-roberta500k-helpful</a></strong> to <a href='https://wandb.ai/luka-krsnik-outsmartify/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/luka-krsnik-outsmartify/huggingface' target=\"_blank\">https://wandb.ai/luka-krsnik-outsmartify/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/luka-krsnik-outsmartify/huggingface/runs/o0no16wd' target=\"_blank\">https://wandb.ai/luka-krsnik-outsmartify/huggingface/runs/o0no16wd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luka/Development/personal/steam-experiments/venv2/lib/python3.10/site-packages/transformers/trainer.py:2833: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_rng_state = torch.load(rng_file)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='31250' max='31250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [31250/31250 4:00:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Mae</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>18750</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.003207</td>\n",
       "      <td>0.003207</td>\n",
       "      <td>0.020156</td>\n",
       "      <td>-0.062829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21875</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.002936</td>\n",
       "      <td>0.002936</td>\n",
       "      <td>0.019235</td>\n",
       "      <td>0.026894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.002887</td>\n",
       "      <td>0.002887</td>\n",
       "      <td>0.014255</td>\n",
       "      <td>0.043331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28125</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.002932</td>\n",
       "      <td>0.002932</td>\n",
       "      <td>0.013278</td>\n",
       "      <td>0.028268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31250</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.002884</td>\n",
       "      <td>0.002884</td>\n",
       "      <td>0.014959</td>\n",
       "      <td>0.044279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=31250, training_loss=0.0021845314712524416, metrics={'train_runtime': 14445.8354, 'train_samples_per_second': 34.612, 'train_steps_per_second': 2.163, 'total_flos': 1.1164339834883568e+17, 'train_loss': 0.0021845314712524416, 'epoch': 1.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trainer.train()\n",
    "trainer.train(resume_from_checkpoint = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ea400f1-0fe3-4366-a27a-bb6aa0041a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 = I totally agree the best bit of the sims making dream homes, but really needs the ability to sell and make a profit so you can buy more land and build your housing empire. Love the designs and the possibilities for the game\n",
      "1 = Banger game cheap when on sale comes with a lot of dlc! Start the exe from the main folder and it won't crash because \"ran out of memory\" as much and enjoy pasting all those codes for the keys. I think 7 hours out of my 12 is just pasting codes.\n",
      "0 = Too much random ♥♥♥♥♥♥♥♥.\n",
      "1 = Story - 8\n",
      "Visuals - 9\n",
      "Audio - 9\n",
      "Gameplay - 8\n",
      "Length - 6\n",
      "Replayability - 7\n",
      "Value base $ - 6\n",
      "Value sale ($3.74) - 8\n",
      "Overall - 8\n",
      "1 = This game is awesome. Play it.\n",
      "1 = Great story, a visual novel walking simulator which is fueled by the modern non-organic extraterrestrial travelling theories. Better than most movies i guess..\n",
      "Oh! It ran well, with full details with my 7 years old 1070GTX card, exhibiting me a visually stunning red planet\n",
      "1 = amazing game the artstyle is creepy the controls are awesome and basically everything is cool\n",
      "1 = Some really tedious game mechanics (even by RGG standards - e.g. endless tailing missions, fiddling with keys, etc.) with mostly below average side quests. However, the main story and the accompanying soundtrack is excellent and more than makes up for these flaws. This would probably be an even better game overall if it wasn't chained back by the usual RGG elements which did little to improve the quality of the game anyway.\n",
      "1 = I really enjoy this game. If you want a FPS that takes elements of CoD and CSGO, and puts them into a F2P game, this is worth checking out. Might not be perfect, but I mean c'mon it's still early access. Play it, and take it for what it is. It is still a developing game. It has great potential. It runs well on my old toaster laptop, has fun game modes, and balanced guns, cool maps, so I vote yes for Black Sqaud, despite its minor flaws.\n",
      "1 = > Got the game out of pure curiosity\n",
      "> Hmm, the artwork and soundtrack are pretty nice\n",
      "> Get outside for the first time\n",
      "> HOLY ♥♥♥♥♥ IT'S SO MESMERIZING AND SORROWFUL AT THE SAME TIME, WTH\n",
      "> Yeah, Mili did the soundtrack, so that's why\n",
      "> Tbh, the combat and exploration is not that varied, huh\n",
      "> Regret saying so cuz later, the map is so large and overwhelming that it made me feel helpless at times\n",
      "> The combat got so much better after each boss fight. Srsly, progressing in this game feels so satisfying.\n",
      "> The story and the lore... man... it adds so much to the game. I just read the some left behind notes and im sad af rn.\n",
      "> At some point, reach Verboten Domain\n",
      "> Feel the dread and fear constantly\n",
      "> Reach the Abyss\n",
      "> SH** MY PANTS WHILE SUFFOCATING IN THE GAS CHAMBER\n",
      "> Reach final boss\n",
      "> Cry\n",
      "10/10 would recommend playing it all again\n"
     ]
    }
   ],
   "source": [
    "model.to('cuda')\n",
    "def example_predictions(dataset, model):\n",
    "    for text in dataset:\n",
    "        inputs = tokenizer.encode(text, return_tensors=\"pt\").to('cuda')\n",
    "        logits = model(inputs).logits\n",
    "        predictions = torch.max(logits, 1).indices\n",
    "    \n",
    "        print(f'{predictions.tolist()[0]} = {text}')\n",
    "example_predictions(tokenized_dataset['dev']['text'][:10], model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb756047-457c-4208-b74f-e042a873e188",
   "metadata": {},
   "source": [
    "# Pytorch training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7984e142-ab2a-4510-ba16-03e006e6ea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_function(data):\n",
    "#     text, tokenizer = data\n",
    "#     return tokenizer(text, padding=\"max_length\", truncation=True, return_tensors=\"pt\", max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "533ffa51-fa8c-4dd4-802d-768a7785bd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import Pool, cpu_count\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# def tokenize_text(df, chunksize=500):\n",
    "#     text = list(df['text'])\n",
    "#     data = [\n",
    "#         (text[i: i + chunksize], tokenizer) for i in range(0, len(text), chunksize)\n",
    "#     ]\n",
    "#     with Pool(16) as p:\n",
    "#         examples = list(\n",
    "#             tqdm(\n",
    "#                 p.imap(tokenize_function, data),\n",
    "#                 total=len(text) // chunksize,\n",
    "#                 disable=False,\n",
    "#             )\n",
    "#         )\n",
    "\n",
    "#     return examples\n",
    "\n",
    "# def prepare_data(df, examples, output_mode=\"classification\"):\n",
    "#     examples = {\n",
    "#         key: torch.cat([example[key] for example in examples])\n",
    "#         for key in examples[0]\n",
    "#     }\n",
    "#     if output_mode == \"classification\":\n",
    "#         labels = torch.tensor(list(df['label']), dtype=torch.long)\n",
    "#     elif output_mode == \"regression\":\n",
    "#         labels = torch.tensor(list(df['label']), dtype=torch.float)\n",
    "#     return examples, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66a343d4-72a2-49af-aca5-8f5e88343ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c49c7142c424689827943d87dcdef9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18f979bc3234266bb91d7d4afd8e503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train_tokenized = tokenize_text(df_train)\n",
    "# dev_tokenized = tokenize_text(df_dev)\n",
    "\n",
    "# # text has to be tokenized before further processing due to multiprocessing having problems when combined with torch (and possibly other libraries connected with GPU processing)\n",
    "# # A workaround is to use `multiprocessing.set_start_method('spawn')` but this is impractical in jupyter\n",
    "# # Avoiding multiprocessing might work but is prob. slower.\n",
    "# train = prepare_data(df_dev, train_tokenized)\n",
    "# dev = prepare_data(df_dev, dev_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f985be-de8a-4c9c-9813-ddaf6434cd1c",
   "metadata": {},
   "source": [
    "# PyTorch (semi)-implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74ac2cba-8dd2-4a9d-8c10-be85ee7927e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "train_dataset = (examples, labels)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=train_sampler,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c98741-7fae-41d4-868b-312142463e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST:\n",
    "model = AutoModelForSequenceClassification.from_pretrained('distilbert/distilbert-base-uncased', num_labels=2)\n",
    "optimizer = torch.optim.adamw.AdamW(model.parameters(), lr=4e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6627cfcf-0abc-40f1-9f51-a465da9e0021",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = trange(\n",
    "    1, desc=\"Epoch\", disable=False, mininterval=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94fa03a-5361-4d78-a9cb-3140625189db",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_number = 0\n",
    "for _ in train_iterator:\n",
    "    model.train()\n",
    "    train_iterator.set_description(\n",
    "        f\"Epoch {epoch_number + 1} of {args.num_train_epochs}\"\n",
    "    )\n",
    "    batch_iterator = tqdm(\n",
    "        train_dataloader,\n",
    "        desc=f\"Running Epoch {epoch_number + 1} of {1}\",\n",
    "        disable=False,\n",
    "        mininterval=0,\n",
    "    )\n",
    "    for step, batch in enumerate(batch_iterator):\n",
    "        print(batch)\n",
    "        # create inputs = {'input_ids': tensor, 'attention_mask': ..., 'labels': ...\n",
    "        break\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        current_loss = loss.item()\n",
    "        batch_iterator.set_description(\n",
    "            f\"Epochs {epoch_number + 1}/1. Running Loss: {current_loss:9.4f}\"\n",
    "        )\n",
    "        # if necessary scale! - scaler.scale(loss).backward()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "        \n",
    "        \n",
    "    epoch_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb1b7817-412c-4dc0-856b-be1c80974825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST:\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(1):\n",
    "    for batch in dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee3bb23e-9c0d-4701-bcd6-610e85e38874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 2023, 2003, 1037, 2460, 3231, 2005, 1052, 2638, 2819, 17175, 11314, 6444, 2594, 7352, 26461, 27572, 11261, 6767, 15472, 6761, 8663, 10735, 2483, 999, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_text = tokenizer(\"This is a short test for pneumonoultramicroscopicsilicovolcanoconiosis!\")\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f57fd5f-ad84-4c34-91af-844ed3140078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] this is a short test for pneumonoultramicroscopicsilicovolcanoconiosis! [SEP]'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_text[\"input_ids\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
