{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hidden-network",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "manual_seed = 23\n",
    "\n",
    "np.random.seed(manual_seed)\n",
    "pl.set_random_seed(manual_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5f6276-14ce-467b-acc9-5d9a8bcf25f1",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795127bb-4fd4-490e-b58d-8751f5fbbcf8",
   "metadata": {},
   "source": [
    "If necessary, adapt sqlite columns to VARCHAR by:\n",
    "```\n",
    "ALTER TABLE product ADD COLUMN new_price VARCHAR;\n",
    "UPDATE product SET new_price = price;\n",
    "ALTER TABLE product DROP COLUMN price;\n",
    "ALTER TABLE product RENAME COLUMN new_price TO price;\n",
    "```\n",
    "```\n",
    "ALTER TABLE review ADD COLUMN new_found_funny VARCHAR;\n",
    "UPDATE review SET new_found_funny = found_funny;\n",
    "ALTER TABLE review DROP COLUMN found_funny;\n",
    "ALTER TABLE review RENAME COLUMN new_found_funny TO found_funny;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb710fd-a7e6-43ea-944b-0671436b32fd",
   "metadata": {},
   "source": [
    "## Selecting Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bab5b78-a47f-4b45-899e-613326da50d9",
   "metadata": {},
   "source": [
    "## Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e93cf99-5fdf-42ca-b6b9-f34d225ce816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_path = '../dbs/db_micro2.sqlite3'\n",
    "db_path = '../dbs/db2.sqlite3'\n",
    "connection_string = 'sqlite://' + db_path\n",
    "df = pl.read_database_uri(\n",
    "    '''SELECT product_id, text AS review_text, recommended, found_helpful, found_funny \n",
    "    FROM review LEFT JOIN product ON product_id = product.id''',\n",
    "    connection_string\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa182d04-fd71-4842-88d2-8632a6ff1f9b",
   "metadata": {},
   "source": [
    "### Formatting data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613bf077-212b-42ad-9065-1d03b727ee52",
   "metadata": {},
   "source": [
    "#### Convert to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77ea6b4f-55e5-4a3c-8626-9283ad14d9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to integers\n",
    "df = df.with_columns(\n",
    "    # cast features to minimal viable types\n",
    "    pl.col(\"found_funny\").cast(pl.UInt16, strict=False).fill_null(strategy=\"zero\"),\n",
    "    pl.col(\"found_awarding\").cast(pl.UInt16, strict=False).fill_null(strategy=\"zero\"),\n",
    "    pl.col(\"found_helpful\").cast(pl.UInt16, strict=False).fill_null(strategy=\"zero\"),\n",
    "    pl.col(\"recommended\").cast(pl.Int8)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b129fad-044a-438f-ab24-6d642b3aa9f3",
   "metadata": {},
   "source": [
    "#### Choosing and calculating regression metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54065551-9489-4c44-bd7a-51e230b7a26f",
   "metadata": {},
   "source": [
    "We are trying to predict how funny, helpful, and awarding a review is. We have insight into this because people on Steam vote for reviews in each category. The easiest metric we could use is to try to predict how many people found a review funny. The problem is, that some reviews have more views than others, so it is only logical that those viewed less have fewer votes. Unfortunately, we don't have information about the number of views of a review, so we have to find another way to consider this metric.\n",
    "\n",
    "We could do this by normalizing each metric by product. We would have values between 0 and 1, where 0 would indicate no votes for that review and 1 would tell us that this is the most upvoted product review. This metric assumes, that all reviews of specific products had equal opportunity to be upvoted. This is probably related to numerous other factors, like which comments are highlighted by Steam, how many people were active when that review was written etc., but it still gives us some insight into how well a review was written for a specific product. The downside is that products with few views are treated equally as products with many views. This is problematic because it is much harder to write the best (or close to the best) review when there are more reviews. Values in less popular products would therefore be much higher than others.\n",
    "\n",
    "So far we proposed two metrics, one that evaluates reviews overall, ignoring that some may be less viewed than others, and the second that acknowledges this fact but skews less viewed votes upwards. Is it possible to get a better metric that is a compromise of the two? To do this I propose a metric that is calculated using information about how upvoted a review was to others of the same product, and all other reviews. It should regard both values equally, so it is calculated as their sum, divided by two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4492cb1a-14b5-4dd0-8031-463138029082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_column(df: pl.DataFrame, column_name: str, quantile: float=0.999, new_column_name: str=None) -> (pl.DataFrame, float):\n",
    "    \"\"\"\n",
    "    When a value in a specified column falls outside the specified quantile, make it equal to the largest value in the specified quantile.\n",
    "    This is used to clip big outliers for \n",
    "    \"\"\"\n",
    "    if new_column_name is None:\n",
    "        new_column_name = column_name\n",
    "    cutoff_value = df.select(column_name).quantile(0.999)\n",
    "    return pl.when(pl.col(column_name) > cutoff_value).then(cutoff_value).otherwise(pl.col(column_name)).alias(new_column_name), cutoff_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06220fdb-fbee-4058-806d-1b47f110e49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "found_funny_expr, found_funny_cutoff_value = clip_column(df, 'found_funny', new_column_name='found_funny_cutoff')\n",
    "found_awarding_expr, found_awarding_cutoff_value = clip_column(df, 'found_awarding', new_column_name='found_awarding_cutoff')\n",
    "found_helpful_expr, found_helpful_cutoff_value = clip_column(df, 'found_helpful', new_column_name='found_helpful_cutoff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef3d595c-a3be-4d7e-802b-0e50c6171d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create normalized metrics\n",
    "df = df.with_columns(\n",
    "    found_funny_expr,\n",
    "    found_awarding_expr,\n",
    "    found_helpful_expr\n",
    ")\n",
    "df = df.with_columns(\n",
    "    (\n",
    "        (\n",
    "            (pl.col(\"found_funny_cutoff\") / pl.col(\"found_funny_cutoff\").max()) + \n",
    "            (pl.col(\"found_funny_cutoff\") / pl.col(\"found_funny_cutoff\").max()).over(\"product_id\")\n",
    "        ) / 2).fill_nan(0.0).alias(\"found_funny\"),\n",
    "    (\n",
    "        (\n",
    "            (pl.col(\"found_helpful_cutoff\") / pl.col(\"found_helpful_cutoff\").max()) + \n",
    "            (pl.col(\"found_helpful_cutoff\") / pl.col(\"found_helpful_cutoff\").max()).over(\"product_id\")\n",
    "        ) / 2).fill_nan(0.0).alias(\"found_helpful\"),\n",
    "    (\n",
    "        (\n",
    "            (pl.col(\"found_awarding_cutoff\") / pl.col(\"found_awarding_cutoff\").max()) + \n",
    "            (pl.col(\"found_awarding_cutoff\") / pl.col(\"found_awarding_cutoff\").max()).over(\"product_id\")\n",
    "        ) / 2).fill_nan(0.0).alias(\"found_awarding\")\n",
    ")\n",
    "df = df.drop([\"found_funny_cutoff\", \"found_helpful_cutoff\", \"found_awarding_cutoff\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87fe91f-8d61-4fba-a850-7e8522157494",
   "metadata": {},
   "source": [
    "# Investigating review_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d72ccc49-27e0-4453-9bd4-7db691e43eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
      "\"Mr.Drippy got the drip sheeeeeeeeeesh\"\n",
      "\"useful\"\n",
      "\"A fun yet short semi-rogue like game with a pixel art style. Your goal is to traverse the world and defeat 5 bosses, all the while inventing new tools and upgrading yourself to become stronger and learning more about the world. I was able to 100% the game in 7 hours, though there is an extra difficulty you can unlock. Controller support is available, and I found the controls very smooth and nice when playing. There are a few things the game doesn't teach you in the tutorial, like that you have a map you can open by hitting select, but otherwise I found it enjoyable while playing, even if it was a bit on the easier side. You can definitely make significant progress in the game in 2 hours, so if you have the money to spare to buy it, I'd recommend you try it out for a bit, and if you don't like it you can always return it.\"\n",
      "\"Great game. Feels like a blending of Borderlands, Mass Effect, and Fallout 4. I bought it for $30 and it's a great deal at that price!\"\n",
      "\"Uma Hist√≥ria simples mais interessante vale a pena gastar horas jogano ;D .\"\n",
      "\"Tons of content this is a game that can be played 300 hours and still not finished.\n",
      "So many things you can do besides the main quests\"\n",
      "\"I really enjoy this game. The graphics are nice along with the combat animations. It has enough depth to keep me interested and the crafting is a lot of fun. I'm really puzzled on why the reviews are not better but to each their own.\"\n",
      "\"seru ini game , build nya satu satu motong ilalang\"\n",
      "\"It's okay, I guess.\"\n",
      "\"Love it!!\"\n"
     ]
    }
   ],
   "source": [
    "print(list(string.ascii_lowercase) + list(string.ascii_uppercase))\n",
    "i = 0\n",
    "for review in df.select(['review_text']).sample(10).iter_rows():\n",
    "    print(f'\"{review[0]}\"')\n",
    "    #print(review[0].contains(list(string.ascii_lowercase) + list(string.ascii_uppercase)))\n",
    "    i += 1\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cdb5a25-6ac0-4815-a161-6711d8b94ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out reviews that don't contain any letters.\n",
    "df = df.filter(pl.col('review_text').str.contains_any(list(string.ascii_lowercase) + list(string.ascii_uppercase)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "158d4086-b622-415d-a622-5e92803869b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train test dev\n",
    "df_split = df.select(\"product_id\").unique(\"product_id\").sort(\"product_id\")\n",
    "df_split = df_split.with_columns(\n",
    "    pl.lit(np.random.rand(df_split.height)).alias(\"split\")\n",
    ")\n",
    "df_split = df_split.with_columns(\n",
    "    pl.when(pl.col(\"split\") < 0.8).then(pl.lit(\"train\")).otherwise(pl.when(pl.col(\"split\") < 0.9).then(pl.lit(\"test\")).otherwise(pl.lit(\"dev\"))).alias(\"split\")\n",
    ")\n",
    "df_dict = df.join(df_split, on=\"product_id\", how=\"left\").partition_by(\"split\", as_dict=True, include_key=False)\n",
    "# df_split[\"split\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12c7ed9e-03e4-4f3b-a8da-d3a357b7d03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33904649\n",
      "4614750\n",
      "4643683\n"
     ]
    }
   ],
   "source": [
    "print(df_dict[(\"train\",)].shape[0])\n",
    "print(df_dict[(\"test\",)].shape[0])\n",
    "print(df_dict[(\"dev\",)].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4aae524d-684e-4e5a-a62d-d10051aa26a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_parquet(df_dict, filename):\n",
    "    df_dict[(\"train\",)].write_parquet(filename + '_train.parquet')\n",
    "    df_dict[(\"test\",)].write_parquet(filename + '_test.parquet')\n",
    "    df_dict[(\"dev\",)].write_parquet(filename + '_dev.parquet')\n",
    "\n",
    "write_parquet(df_dict, \"data/complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "371db6ac-7a0d-4cf5-bcaa-c20b8e6f4a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict_500k = {}\n",
    "df_dict_500k[(\"train\",)] = df_dict[(\"train\",)].sample(500000, seed=manual_seed, shuffle=True)\n",
    "df_dict_500k[(\"test\",)] = df_dict[(\"test\",)].sample(50000, seed=manual_seed, shuffle=True)\n",
    "df_dict_500k[(\"dev\",)] = df_dict[(\"dev\",)].sample(50000, seed=manual_seed, shuffle=True)\n",
    "\n",
    "write_parquet(df_dict_500k, \"data/500k_50k\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
